{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7514b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import psycopg2\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set custom path for secrets\n",
    "sys.path.insert(0,\"/home/nuclear/Github/PythonPrograms/Secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d10938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "import crawler_pg_secrets as p\n",
    "\n",
    "pg_params = {\n",
    "    'host' : p.host,\n",
    "    'user' : p.user,\n",
    "    'dbname' : p.dbname,\n",
    "    'password' : p.password,\n",
    "    'port' : p.port\n",
    "}\n",
    "\n",
    "conn =  psycopg2.connect(**pg_params)\n",
    "conn.commit()\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b37d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: drop table indeed_horizon;\n",
      "success: create table indeed_horizon (id serial, url varchar(256), visited integer);\n"
     ]
    }
   ],
   "source": [
    "table_name = 'indeed_horizon'\n",
    "\n",
    "# Drop previous horizon table\n",
    "sql_cmnd = \"drop table \" + table_name + \";\"\n",
    "try:\n",
    "    cur.execute(sql_cmnd)\n",
    "    print('success: ' + sql_cmnd)\n",
    "except:\n",
    "    print('FAIL: ' + sql_cmnd)\n",
    "conn.commit()\n",
    "\n",
    "# Create new horizon table\n",
    "sql_cmnd = \"create table \"\\\n",
    "    + table_name\\\n",
    "    + \" (id serial, url varchar(256), visited integer);\"\n",
    "try:\n",
    "    cur.execute(sql_cmnd)\n",
    "    print('success: ' + sql_cmnd)\n",
    "except:\n",
    "    print('FAIL: ' + sql_cmnd)\n",
    "conn.commit()\n",
    "\n",
    "# Add starting url to table\n",
    "url='https://ca.indeed.com'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2047d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0\n",
      "{'Date': 'Fri, 11 Nov 2022 07:20:29 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'close', 'CF-Chl-Bypass': '1', 'Referrer-Policy': 'same-origin', 'Permissions-Policy': 'accelerometer=(),autoplay=(),camera=(),clipboard-read=(),clipboard-write=(),fullscreen=(),geolocation=(),gyroscope=(),hid=(),interest-cohort=(),magnetometer=(),microphone=(),payment=(),publickey-credentials-get=(),screen-wake-lock=(),serial=(),sync-xhr=(),usb=()', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'X-Frame-Options': 'SAMEORIGIN', 'Server-Timing': 'cf-q-config;dur=5.9999983932357e-06', 'Report-To': '{\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report\\\\/v3?s=5%2BTQJK6TZpQRR%2FVHpXpa%2BZqVwyDopPsjimM6j8HgKPo5yUrgjwumA4BrI%2FQ8Z0nEwaSj7%2B1t56r2ll8rhuyxRUO6W519BQmRwkL5i5vCUe%2BvnXdy80Wf0YuoW7Qe2zs%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}', 'NEL': '{\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}', 'Vary': 'Accept-Encoding', 'Server': 'cloudflare', 'CF-RAY': '768543fe6be53ff2-YYZ', 'Content-Encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}\n",
      "403\n"
     ]
    }
   ],
   "source": [
    "# Set up requests with headers to bypass restrictions\n",
    "headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0'}\n",
    "print(headers['User-Agent'])\n",
    "# r=requests.get(url, headers=headers)\n",
    "\n",
    "url='https://stomarket.com/market'\n",
    "r=requests.get(url,headers=headers)\n",
    "\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get(url)\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "r = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "print(r.headers)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b73ceac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Sample code from https://github.com/asifexplore/Indeed-Web-Crawler/blob/master/indeed_crawler_upgrade.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "idArray = []\n",
    "\n",
    "# Extracts everything from the page. Entire HTML \n",
    "def extractLayer1(page):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0'}\n",
    "    url = f'https://www.indeed.com/jobs?q=Developer+USA&explvl=entry_level&start={page}'\n",
    "    r = requests.get(url,headers)\n",
    "    print(r.status_code)\n",
    "    # Used to return status 200 if we are able to access the site properly. \n",
    "    # return r.status_code\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Filters out the required \"divs\" from HTML (Passed from extract function)\n",
    "def scrapLayer1(soup):\n",
    "    # Finds all the div that has the respective class from the html taken from extract function. \n",
    "    divs = soup.find_all('a',class_ = 'tapItem')\n",
    "    for item in divs: \n",
    "        if item.get('id') is not None:\n",
    "            id = item.get('id')\n",
    "            id = id.replace('job_','')\n",
    "            idArray.append(id)\n",
    "            link = f\"https://www.indeed.com/jobs?q=Developer+USA&explvl=entry_level&vjk={id}\"\n",
    "        else:\n",
    "            idArray.append(\"NTH\")\n",
    "        \n",
    "        title = location = item.find('h2', class_ = 'jobTitle').text.strip()\n",
    "        company = item.find('span', class_ = 'companyName').text.strip()\n",
    "        date = item.find('span', class_ = 'date').text.strip()\n",
    "        summary = item.find('div', class_ = 'job-snippet').text.strip().replace('\\n','')\n",
    "        \n",
    "        location = item.find('div', class_ = 'companyLocation')\n",
    "        if location is not None:\n",
    "            location = location.text.strip()\n",
    "        else:\n",
    "            location = ''\n",
    "        \n",
    "        # Storing all data obtained into a Dictionary. \n",
    "        job = {\n",
    "            'Job Title':title, \n",
    "            'Job Location':location, \n",
    "            'Job Company Name':company,\n",
    "            'Job Description':summary, \n",
    "            'Job Date' : date, \n",
    "            # 'Job Salary':salary, \n",
    "            'Job Link':link,\n",
    "        }\n",
    "\n",
    "        joblist.append(job)\n",
    "    return \n",
    "\n",
    "# Extracts everything from the page. Entire HTML \n",
    "def extract(id):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    url = f'https://www.indeed.com/viewjob?jk={id}'\n",
    "    r = requests.get(url,headers)\n",
    "    # Used to return status 200 if we are able to access the site properly. \n",
    "    # return r.status_code\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    return soup\n",
    "\n",
    "joblist = []\n",
    "\n",
    "# Get all HTML details of job search \n",
    "# Create loop here to obtain more job info, to cover pagination. \n",
    "c = extractLayer1(0)\n",
    "# Getting ID of all available jobs extracted\n",
    "scrapLayer1(c)\n",
    "\n",
    "for i in range(0,len(idArray)):\n",
    "    d = extract(idArray[i])\n",
    "    desc = d.find('div',class_ = 'jobsearch-jobDescriptionText').text\n",
    "    joblist[i]['Job Description'] = desc\n",
    "    print(joblist[i][\"Job Description\"])\n",
    "\n",
    "df = pd.DataFrame(joblist)\n",
    "print(df.head())\n",
    "df.to_csv(r'C:\\Users\\Asif\\Desktop\\Indeed Crawler\\jobs.csv')\n",
    "print(joblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6e4e14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your skill: analyst\n",
      "Enter preferred location: ontario\n",
      "Enter # number of pages to scrape: 1\n",
      "403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_skill = input('Enter your skill: ').strip()\n",
    "place = input('Enter preferred location: ').strip()\n",
    "no_of_pages = int(input('Enter # number of pages to scrape: '))\n",
    "\n",
    "indeed_post = []\n",
    "\n",
    "for page in range(no_of_pages):\n",
    "    url = 'https://www.indeed.com/jobs?q=data%20analyst&l=US&fromage=1&vjk=91152078d1e41826' + input_skill + \\\n",
    "    '&1=' + place +'&sort=date' + '&start=' + str(page*10)\n",
    "    \n",
    "#request indeed url    \n",
    "    html_content = requests.get(url)\n",
    "    print(html_content.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:web_crawler] *",
   "language": "python",
   "name": "conda-env-web_crawler-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
